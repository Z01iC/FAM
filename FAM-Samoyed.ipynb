{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hMpwz7dPx46"
      },
      "source": [
        "## Pretrained AlexNet\n",
        "\n",
        "https://pytorch.org/vision/stable/models.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVaUZCnVNP35"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "from typing import DefaultDict, Tuple, List\n",
        "from functools import partial\n",
        "\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
        "\n",
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA5UU_ujE17-"
      },
      "source": [
        "##Creating labeled feature patch from test image manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OveNP7cqYeUZ"
      },
      "outputs": [],
      "source": [
        "## Download images\n",
        "!gdown --id '1BwhagAYZlG1cnPZn1N3GTH4Hj2MWliNG'\n",
        "!unzip Samoyed.zip\n",
        "## Download ImageNet labels\n",
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXgvQeIVPzqw"
      },
      "outputs": [],
      "source": [
        "## preprocess image\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def black_image_outside_patch(img, top_left_x, top_left_y, bot_right_x, bot_right_y):\n",
        "    \"\"\"Blacks image outside provided bounding box\n",
        "    ----------\n",
        "    img:\n",
        "        image tensor\n",
        "\n",
        "    top_left_x, top_left_y, bot_right_x, bot_right_y:\n",
        "        coordinates of bounding box for patch to keep\n",
        "    Returns\n",
        "    -------\n",
        "    img_copy:\n",
        "        copy of the image with everything blacked out besides \n",
        "    \"\"\"\n",
        "    img_copy = img.detach().clone()\n",
        "    img_copy[:,:,:top_left_x] = 0\n",
        "    img_copy[:,:,bot_right_x:] = 0\n",
        "    img_copy[:,:top_left_y,:] = 0\n",
        "    img_copy[:,bot_right_y:,:] = 0\n",
        "    return img_copy\n",
        "\n",
        "def random_noise_outside_patch(img, top_left_x, top_left_y, bot_right_x, bot_right_y):\n",
        "    \"\"\"randomizes all pixels outside provided bounding box\n",
        "    ----------\n",
        "    img:\n",
        "        image tensor\n",
        "\n",
        "    top_left_x, top_left_y, bot_right_x, bot_right_y:\n",
        "        coordinates of bounding box for patch to keep\n",
        "    Returns\n",
        "    -------\n",
        "    img_copy:\n",
        "        copy of the image with everything blacked out besides \n",
        "    \"\"\"\n",
        "    img_copy = img.detach().clone()\n",
        "    img_copy[:,:,:top_left_x] = torch.randn(img[:,:,:top_left_x].size())\n",
        "    img_copy[:,:,bot_right_x:] = torch.randn(img[:,:,bot_right_x:].size())\n",
        "    img_copy[:,:top_left_y,:] = torch.randn(img[:,:top_left_y,:].size())\n",
        "    img_copy[:,bot_right_y:,:] = torch.randn(img[:,bot_right_y:,:].size())\n",
        "    return img_copy\n",
        "\n",
        "def show(img):\n",
        "    \"\"\"shows the input img with pyplot\n",
        "    ----------\n",
        "    img:\n",
        "        image tensor\n",
        "    \"\"\"\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhlMk68wYwE-"
      },
      "outputs": [],
      "source": [
        "faces = []\n",
        "noses = []\n",
        "eyes = []\n",
        "mouths = []\n",
        "ears = []\n",
        "furs = []\n",
        "legs = []\n",
        "\n",
        "def append_features(face, nose, eye, mouth, ear, fur, leg):\n",
        "    \"\"\"\n",
        "    appends features to feature lists\n",
        "    \"\"\"\n",
        "    if face is not None:\n",
        "        faces.append(face.to(device))\n",
        "    if nose is not None:\n",
        "        noses.append(nose.to(device))\n",
        "    if eye is not None:\n",
        "        eyes.append(eye.to(device))\n",
        "    if mouth is not None:\n",
        "        mouths.append(mouth.to(device))\n",
        "    if ear is not None:\n",
        "        ears.append(ear.to(device))\n",
        "    if fur is not None:\n",
        "        furs.append(fur.to(device))\n",
        "    if leg is not None:\n",
        "        legs.append(leg.to(device))\n",
        "\n",
        "# manually label all the test feature patches\n",
        "filename = str(1) + '.jpeg'\n",
        "input_image = Image.open(filename)\n",
        "input_tensor = preprocess(input_image)\n",
        "face = random_noise_outside_patch(input_tensor, 50, 0, 150, 100).unsqueeze(0)\n",
        "nose = random_noise_outside_patch(input_tensor, 88, 50, 105, 70).unsqueeze(0)\n",
        "eye = random_noise_outside_patch(input_tensor, 100, 35, 125, 55).unsqueeze(0)\n",
        "mouth = random_noise_outside_patch(input_tensor, 82, 68, 120, 90).unsqueeze(0)\n",
        "ear = random_noise_outside_patch(input_tensor, 102, 0, 145, 30).unsqueeze(0)\n",
        "fur = random_noise_outside_patch(input_tensor, 100, 100, 135, 135).unsqueeze(0)\n",
        "leg = random_noise_outside_patch(input_tensor, 100, 150, 150, 224).unsqueeze(0)\n",
        "append_features(face, nose, eye, mouth, ear, fur, leg)\n",
        "\n",
        "filename = str(2) + '.jpeg'\n",
        "input_image = Image.open(filename)\n",
        "input_tensor = preprocess(input_image)\n",
        "face = random_noise_outside_patch(input_tensor, 100, 5, 220, 135).unsqueeze(0)\n",
        "nose = random_noise_outside_patch(input_tensor, 150, 80, 180, 110).unsqueeze(0)\n",
        "eye = random_noise_outside_patch(input_tensor, 165, 60, 190, 85).unsqueeze(0)\n",
        "mouth = random_noise_outside_patch(input_tensor, 140, 108, 190, 134).unsqueeze(0)\n",
        "ear = random_noise_outside_patch(input_tensor, 160, 0, 230, 60).unsqueeze(0)\n",
        "fur = random_noise_outside_patch(input_tensor, 50, 125, 100, 150).unsqueeze(0)\n",
        "leg = random_noise_outside_patch(input_tensor, 150, 150, 224, 224).unsqueeze(0)\n",
        "append_features(face, nose, eye, mouth, ear, fur, leg)\n",
        "\n",
        "\n",
        "filename = str(3) + '.jpeg'\n",
        "input_image = Image.open(filename)\n",
        "input_tensor = preprocess(input_image)\n",
        "face = random_noise_outside_patch(input_tensor, 110, 0, 190, 80).unsqueeze(0)\n",
        "nose = random_noise_outside_patch(input_tensor, 140, 50, 170, 65).unsqueeze(0)\n",
        "eye = random_noise_outside_patch(input_tensor, 155, 28, 178, 48).unsqueeze(0)\n",
        "ear = random_noise_outside_patch(input_tensor, 145, 0, 180, 30).unsqueeze(0)\n",
        "fur = random_noise_outside_patch(input_tensor, 60, 30, 90, 75).unsqueeze(0)\n",
        "leg = random_noise_outside_patch(input_tensor, 20, 100, 60, 224).unsqueeze(0)\n",
        "append_features(face, nose, eye, mouth, ear, fur, leg)\n",
        "\n",
        "filename = str(4) + '.jpeg'\n",
        "input_image = Image.open(filename)\n",
        "input_tensor = preprocess(input_image)\n",
        "face = random_noise_outside_patch(input_tensor, 110, 35, 200, 110).unsqueeze(0)\n",
        "nose = random_noise_outside_patch(input_tensor, 146, 75, 162, 90).unsqueeze(0)\n",
        "eye = random_noise_outside_patch(input_tensor, 155, 65, 172, 78).unsqueeze(0)\n",
        "mouth = random_noise_outside_patch(input_tensor, 140, 86, 170, 105).unsqueeze(0)\n",
        "ear = random_noise_outside_patch(input_tensor, 160, 35, 190, 60).unsqueeze(0)\n",
        "fur = random_noise_outside_patch(input_tensor, 130, 110, 170, 140).unsqueeze(0)\n",
        "leg = random_noise_outside_patch(input_tensor, 160, 150, 190, 210).unsqueeze(0)\n",
        "append_features(face, nose, eye, mouth, ear, fur, leg)\n",
        "\n",
        "filename = str(5) + '.jpeg'\n",
        "input_image = Image.open(filename)\n",
        "input_tensor = preprocess(input_image)\n",
        "face = random_noise_outside_patch(input_tensor, 80, 20, 180, 100).unsqueeze(0)\n",
        "nose = random_noise_outside_patch(input_tensor, 97, 54, 110, 72).unsqueeze(0)\n",
        "eye = random_noise_outside_patch(input_tensor, 100, 35, 125, 55).unsqueeze(0)\n",
        "mouth = random_noise_outside_patch(input_tensor, 92, 70, 135, 95).unsqueeze(0)\n",
        "ear = random_noise_outside_patch(input_tensor, 120, 25, 175, 50).unsqueeze(0)\n",
        "fur = random_noise_outside_patch(input_tensor, 100, 100, 135, 135).unsqueeze(0)\n",
        "leg = random_noise_outside_patch(input_tensor, 90, 175, 150, 224).unsqueeze(0)\n",
        "append_features(face, nose, eye, mouth, ear, fur, leg)\n",
        "\n",
        "filename = str(6) + '.jpeg'\n",
        "input_image = Image.open(filename)\n",
        "input_tensor = preprocess(input_image)\n",
        "face = random_noise_outside_patch(input_tensor, 110, 15, 180, 100).unsqueeze(0)\n",
        "nose = random_noise_outside_patch(input_tensor, 137, 62, 154, 78).unsqueeze(0)\n",
        "eye = random_noise_outside_patch(input_tensor, 148, 45, 166, 60).unsqueeze(0)\n",
        "mouth = random_noise_outside_patch(input_tensor, 125, 75, 165, 94).unsqueeze(0)\n",
        "ear = random_noise_outside_patch(input_tensor, 150, 20, 180, 50).unsqueeze(0)\n",
        "fur = random_noise_outside_patch(input_tensor, 150, 100, 180, 125).unsqueeze(0)\n",
        "leg = random_noise_outside_patch(input_tensor, 130, 125, 170, 205).unsqueeze(0)\n",
        "append_features(face, nose, eye, mouth, ear, fur, leg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC4tflybsn_I"
      },
      "source": [
        "## Pytorch Hooks for saving activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7rBmMqssqGK"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "#                    CITATIONS                    \n",
        "# https://www.lyndonduong.com/saving-activations/\n",
        "# https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/#extracting-activations-from-a-layer\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "def save_adaptive_activations(\n",
        "        activations: DefaultDict,\n",
        "        name: str,\n",
        "        module: nn.Module,\n",
        "        inp: Tuple,\n",
        "        out: torch.Tensor\n",
        ") -> None:\n",
        "    \"\"\"PyTorch Forward hook to save outputs and inhibitory hidden state at each forward\n",
        "    pass. Mutates specified dict objects with each fwd pass.\n",
        "    \"\"\"\n",
        "    activations[name].append(out.detach().cpu())\n",
        "\n",
        "\n",
        "def register_activation_hooks(\n",
        "        model: nn.Module,\n",
        ") -> DefaultDict[List, torch.Tensor]:\n",
        "    \"\"\"Registers forward hooks in specified layers.\n",
        "    Parameters\n",
        "    ----------\n",
        "    model:\n",
        "        PyTorch model\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    activations_dict:\n",
        "        dict of lists containing activations of specified layers in the\n",
        "        form (k,v) where k is the name of the layer and v is a list of the \n",
        "        activation tensors in the order that they were run through the nerual \n",
        "        network\n",
        "    \"\"\"\n",
        "    activations_dict = collections.defaultdict(list)\n",
        "    hooks = []\n",
        "    for name, module in model.named_modules():\n",
        "        hooks.append(module.register_forward_hook(\n",
        "            partial(save_adaptive_activations, activations_dict, name)\n",
        "        ))\n",
        "\n",
        "    return activations_dict, hooks\n",
        "\n",
        "def remove_hooks(\n",
        "    hooks: List,\n",
        ") -> None:\n",
        "    \"\"\"Registers forward hooks in specified layers.\n",
        "    Parameters\n",
        "    ----------\n",
        "    hooks:\n",
        "        list of hooks attached to the model\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6iPLafx-6hR"
      },
      "source": [
        "##Calculate the error between saved activation data structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHyS3UEg-5Nn"
      },
      "outputs": [],
      "source": [
        "def matched_activations(activations, train_indices, test_ind, error_func):\n",
        "    \"\"\" Returns what the patch indices that have similar activations to patch\n",
        "    represented by test_ind\n",
        "    ----------\n",
        "    activations:\n",
        "        dict: {'layer name': List of tensors of activations}\n",
        "    \n",
        "    train_indices:\n",
        "        list of indices that represent what indices within the values of \n",
        "        activations dict lists are train indices\n",
        "\n",
        "    test_index:\n",
        "        list of indices that represent what index within the values of \n",
        "        activations dict lists are the test example\n",
        "\n",
        "    error_func:\n",
        "        func (tensor, tensor) -> int that gives error metric between tensors\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matched_indices: (index, error)\n",
        "        sorted list of what train indices match up well with the test index, \n",
        "        sorted from lowest error to highest error\n",
        "    \"\"\"\n",
        "    matched_indices = []\n",
        "\n",
        "    for train_ind in train_indices:\n",
        "        # calculate error between train_ind and test_ind\n",
        "        train_err = 0\n",
        "        for i, name in enumerate(activations.keys()):\n",
        "            train_err += np.exp(2 * i / len(activations)) * error_func(activations[name][train_ind], activations[name][test_ind])\n",
        "        # create mew match tuple\n",
        "        new_matched_el = (train_ind, train_err)\n",
        "\n",
        "        # find index to insert new_matched_el\n",
        "        ind = len(matched_indices)\n",
        "        for i in range(len(matched_indices)):\n",
        "            if new_matched_el[1] < matched_indices[i][1]:\n",
        "              ind = i\n",
        "              break;\n",
        "\n",
        "        # insert new_matched_el into correct place in list\n",
        "        if ind == len(matched_indices):\n",
        "            matched_indices.append(new_matched_el)\n",
        "        else:\n",
        "            matched_indices = matched_indices[:i] + [new_matched_el] + matched_indices[i:]\n",
        "\n",
        "    return matched_indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zupVRuCjZPd"
      },
      "source": [
        "##Test activation similarities between labeled features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfx_pzHCjYmZ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch.nn as nn\n",
        "activations_dict, hooks = register_activation_hooks(model)\n",
        "\n",
        "# run training images through the model\n",
        "num_images = 6\n",
        "num_features = 7\n",
        "num_feature_images = num_images * num_features\n",
        "for face in faces:\n",
        "    model(face)\n",
        "for nose in noses:\n",
        "    model(nose)\n",
        "for eye in eyes:\n",
        "    model(eye)\n",
        "for mouth in mouths:\n",
        "    model(mouth)\n",
        "for ear in ears:\n",
        "    model(ear)\n",
        "for fur in furs:\n",
        "    model(fur)\n",
        "for leg in legs:\n",
        "    model(leg)\n",
        "\n",
        "classes = ['face', 'nose', 'eye', 'mouth', 'ear', 'fur', 'leg']\n",
        "def index_to_class():\n",
        "    \"\"\" Returns a dictionary that map the index of a feature patch\n",
        "    to the class label\n",
        "    ----------\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    index_to_class_dict:\n",
        "        {index: class name}\n",
        "    \"\"\"\n",
        "    index_to_class_dict = {}\n",
        "    for i in range(num_feature_images):\n",
        "        index_to_class_dict[i] = classes[i//num_images]\n",
        "\n",
        "    return index_to_class_dict\n",
        "\n",
        "def class_to_score(scores, index_to_class_dict):\n",
        "    \"\"\" Calculates the loss between the test image and each feature class\n",
        "    ----------\n",
        "    scores:\n",
        "        list of tuples of the form [(ind, loss)] where the training patch\n",
        "        at index has loss loss\n",
        "    \n",
        "    index_to_class_dict:\n",
        "        dictionary that maps indices to classes\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    class_to_scores: \n",
        "        A dictionary that maps a feature class to the total loss\n",
        "    \"\"\"\n",
        "    class_to_scores = {}\n",
        "    class_counters = {}\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_to_scores[class_name] = 0\n",
        "        class_counters[class_name] = 0\n",
        "\n",
        "    for ind, loss in scores:\n",
        "        class_name = index_to_class_dict[ind]\n",
        "        class_to_scores[class_name] += loss\n",
        "        class_counters[class_name] += 1\n",
        "\n",
        "    for class_name, counter in class_counters.items():\n",
        "        class_to_scores[class_name] /= counter\n",
        "\n",
        "    return class_to_scores\n",
        "\n",
        "\n",
        "index_to_class_dict = index_to_class()\n",
        "incorrect_preds = 0\n",
        "\n",
        "# see if the train patches activations match up with each other\n",
        "for i in range(num_feature_images):\n",
        "    test_indices = list(np.arange(num_feature_images))\n",
        "    test_indices = test_indices[:i] + test_indices[i+1:]\n",
        "    matched_activation_results = matched_activations(activations_dict, test_indices, i, nn.L1Loss())\n",
        "    class_to_score_dict = class_to_score(matched_activation_results, index_to_class_dict)\n",
        "    print(f\"{index_to_class_dict[i]}, {i}: {class_to_score_dict}\")\n",
        "    predicted_class = (None, 99999999)\n",
        "    for class1, score in class_to_score_dict.items():\n",
        "        if score < predicted_class[1]:\n",
        "            predicted_class = (class1, score)\n",
        "    if predicted_class[0] != index_to_class_dict[i]:\n",
        "        incorrect_preds += 1\n",
        "    print(f\"prediction for {index_to_class_dict[i]} {i}: {predicted_class[0]}, loss: {predicted_class[1]}\")\n",
        "\n",
        "print(f\"correct predictions for {1 - round(incorrect_preds / num_feature_images,2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSXRgCLGMSJ1"
      },
      "source": [
        "##Extract Potential Features from Test Images\n",
        "\n",
        "Now that we have saved the activations of many features within the training set, we have to attempt to extract features patches from new test images that the CNN recognizes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJZhseo3MVgb"
      },
      "source": [
        "###Naivly testing all possible patches of image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qfXeXjeM48G"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.loss import L1Loss\n",
        "import time\n",
        "\n",
        "#open test image\n",
        "filename = str(10) + '.jpeg'\n",
        "input_image = Image.open(filename)\n",
        "input_tensor = preprocess(input_image).to(device)\n",
        "\n",
        "def garbage_collect(activations):\n",
        "    \"\"\"\n",
        "    Remove the activations stored in the activations dict from the most recent \n",
        "    test image so memory does not overflow\n",
        "    \"\"\"\n",
        "    for k, v in activations.items():\n",
        "        activations[k] = v[:-1]\n",
        "\n",
        "def extract_all_patches(img):\n",
        "    \"\"\"\n",
        "    Extract all the feature patches from img\n",
        "    \"\"\"\n",
        "    train_indices = list(np.arange(num_feature_images))  \n",
        "    width = img.size()[2]\n",
        "    height = img.size()[1]\n",
        "    # dict that maps feature to patch with minimum loss in the form \"class\":(dim, x, y, loss)\n",
        "    feature_mins = {}\n",
        "    for i in range(num_features):\n",
        "        feature_mins[index_to_class_dict[i*num_images]] = (0,0,0,9999999999)\n",
        "    # iterate through all the possible dimensions, and coordinates\n",
        "    for dim in [20, 40, 80]:\n",
        "        for x in range(0, width-dim, 10):\n",
        "            for y in range(0, height-dim, 10):\n",
        "                # create test patch\n",
        "                test_patch = random_noise_outside_patch(img, x, y, x+dim, y+dim)\n",
        "                # run test patch through nn, so hooks save the activations\n",
        "                model(test_patch.unsqueeze(0).to(device))\n",
        "                # Compare the activations between test patch and labeed train patches\n",
        "                matched_activation_results = matched_activations(activations_dict, train_indices, -1, nn.L1Loss())\n",
        "                # calculate the loss for each class\n",
        "                class_to_score_dict = class_to_score(matched_activation_results, index_to_class_dict)\n",
        "                # if the loss is the minimum for the feature class, save it\n",
        "                for feat, loss in class_to_score_dict.items():\n",
        "                    if loss < feature_mins[feat][3]:\n",
        "                        feature_mins[feat] = (dim, x, y, loss)\n",
        "                # delete the activations stored from the most recent test patch\n",
        "                garbage_collect(activations_dict)\n",
        "\n",
        "    return feature_mins\n",
        "\n",
        "start_time = time.time()\n",
        "# expected runtime around 30 minutes, depending on how many candidate patches tested\n",
        "extracted_patches = extract_all_patches(input_tensor)\n",
        "print(time.time() - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki6az8gdaA6b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def show_images(img) -> None:\n",
        "    \"\"\"\n",
        "    View multiple images stored in files, stacking vertically\n",
        "\n",
        "    Arguments:\n",
        "        filename: str - path to filename containing image\n",
        "    \"\"\"\n",
        "    # <something gets done here>\n",
        "    plt.figure()\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "\n",
        "show_images(input_tensor)\n",
        "\n",
        "def print_extracted_patches(extracted_patches, img):\n",
        "    for feature_name, (dim, x, y, loss) in extracted_patches.items():\n",
        "        patch = random_noise_outside_patch(img, x, y, x+dim, y+dim)\n",
        "        print(f\"the extracted patch for {feature_name} is at dim:{dim}, x:{x}, y:{y}, with loss: {loss}\")\n",
        "        show_images(patch)\n",
        "print_extracted_patches(extracted_patches, input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For other experiments..."
      ],
      "metadata": {
        "id": "PgKFU-iymFrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See https://colab.research.google.com/drive/1XSqGRIEAMlYgimccmJpHAZTMgyOwpxj8?usp=sharing for School Bus\n",
        "\n",
        "\n",
        "See https://drive.google.com/file/d/1tuo16a-bOVXhimnooDxGhF5wsHxE6fKo/view?usp=sharing  for results after running the notebook on Kaggle Notebook."
      ],
      "metadata": {
        "id": "22Ala0HFmAzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ggVFTPiTmDBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS-6784_final_proj_vgg_dog.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}